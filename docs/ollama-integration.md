**Ollama本地部署与项目集成实践**

在GeoAnalystBench的技术架构中，Ollama承担着关键的模型运行支撑功能。对其工作机制建立完整认知是推动复现流程顺畅进行的关键所在。

**架构定位与运作原理**

Ollama采用客户端-服务器分离设计，本质上是一套完整的本地LLM运行框架。首次执行任意`ollama`指令时，系统会自动在后台启动HTTP服务进程，默认监听`127.0.0.1:11434`端口。此后该服务持续运行，负责模型加载、推理计算等核心任务。用户在终端输入的各类命令实则充当客户端角色，经由本地接口完成交互。

这种架构带来的直接优势在于，任何具备HTTP通信能力的程序均可调用已部署的模型资源。项目中的Python脚本正是利用这一特性，通过`ollama`库直接与后台服务对接，跳过了命令行界面这一中转层。

**安装配置流程**

各操作系统的安装方式存在差异。macOS与Linux环境可执行官方提供的一键安装脚本，Windows平台则需下载对应的exe安装程序。针对Windows用户，若希望调整程序主体的落盘位置，可在命令行界面通过附加`/DIR`参数的方式实现，例如`.\OllamaSetup.exe /DIR="D:\Ollama"`，从而将接近5GB的核心组件置于非系统分区。完成安装后，通过`ollama pull`指令拉取相应模型，关联文件会自动保存至默认存储路径。

模型文件的存放位置遵循系统约定：Windows环境下位于`C:\Users\<用户名>\.ollama\models`，Linux系统通常为`~/.ollama/models`。由于大型模型占用空间可观，需要调整该路径以避免系统分区容量告急。Windows版本较新的Ollama在图形界面中提供了"Model location"选项，可直接浏览并选定目标文件夹，该方式在当前版本中优先级更高。传统的`OLLAMA_MODELS`环境变量方案在部分场景下可能不被采纳，但仍可作为辅助手段保留。

Windows环境下若搭载NVIDIA显卡，需确保GPU加速机制正常启用。通过新建系统环境变量`OLLAMA_GPU_LAYER`并赋值为`cuda`，同时设置`CUDA_VISIBLE_DEVICES`来选定具体设备（多GPU情况下按`nvidia-smi`输出的编号填写，通常为`0`），即可令模型优先加载至显存而非占用大量内存空间。配置生效需重启Ollama服务，可借助任务管理器或`nvidia-smi`工具验证显存使用情况。

**量化策略与资源需求**

Ollama的模型库采用GGUF量化格式，这是一种将浮点参数压缩为低位整数的技术方案。量化级别直接影响文件体积与推理质量的平衡点：

- Q2/Q3级别可将模型压缩至原始大小的20-30%，但精度损失明显
- Q4量化在体积与性能间达到较优平衡，7B参数模型压缩后约需3.5-4GB存储
- Q5/Q6级别保留更多细节，文件尺寸随之增加至5-7GB范围

运行时的内存占用与磁盘文件大小基本相当，但需额外预留1-2GB空间用于推理中间结果和上下文缓存。以`deepseek-r1:7b`的Q4版本为例，实际运行时建议系统保有5-6GB可用内存。若配备独立显卡，Ollama会优先利用显存加速计算，显存不足时自动回退至系统内存，或采用混合加载策略。

针对16GB内存+4GB显存的典型配置，7B规模的Q4量化模型可流畅运行，但13B及以上参数量的模型会面临显存溢出，系统内存需求突破9GB阈值，运行期间需关闭非必要进程以维持稳定性。

**服务启动与验证**

项目代码中使用的`ollama`库会自动管理后台服务的生命周期。但若需手动操作，可通过以下方式触发服务启动：

```bash
ollama serve
```

该命令会阻塞当前终端并持续输出日志。也可执行任意查询指令如`ollama list`来隐式唤醒服务进程。

验证部署状态最直接的方法是访问API健康检查端点。打开浏览器访问`http://localhost:11434`，若显示"Ollama is running"消息，表明服务已正常就绪。此时即可通过Python代码直接调用已下载的模型，无需预先执行`ollama run`启动交互式会话。

**与项目代码的衔接**

回顾`utils.py`中的`call_ollama`函数实现：

```python
def call_ollama(prompt, model='deepseek-r1:7b', temperature=0.7):
    response = ollama.generate(
        model=model,
        options={"temperature": temperature},
        prompt=prompt
    )
    result = response['response']
    if '</think>' in result:
        return result.split("</think>", 1)[1].strip()
    return result
```

此函数借助`ollama.generate()`方法通过本地API接口发起生成操作，参数`model`指定使用的模型标识符。DeepSeek-R1系列模型会在输出中包裹`<think>...</think>`标记来呈现推理过程，代码逻辑依靠文本分割来提取实际回答内容。这一细节处理确保了返回结果与其他商业模型保持一致的格式规范。

执行`Inference.py`时，脚本会连续执行数百轮模型调用。得益于Ollama服务的持久化运行特性，模型仅在首次调用时加载进内存，后续请求复用已初始化的模型实例，避免了重复加载带来的时间开销。整个推理过程中，Ollama服务进程在后台静默处理所有请求，Python脚本通过库封装的接口完成同步或异步通信，两者协作构成了完整的本地推理链路。